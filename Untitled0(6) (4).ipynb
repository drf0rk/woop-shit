{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D6cJ0h8XvF_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Fixing Gradio Installation for Colab Compatibility\n",
        "\n",
        "print(\"‚ö†Ô∏è Fixing Gradio installation...\")\n",
        "\n",
        "# Uninstall problematic package without touching numpy\n",
        "!pip uninstall -y gradio\n",
        "\n",
        "# Install specific Gradio version without dependencies\n",
        "!pip install gradio==3.41.2 --no-deps\n",
        "\n",
        "# Install some common dependencies that Gradio needs\n",
        "!pip install pydub markdown-it-py mdit-py-plugins\n",
        "\n",
        "# Force reload importlib to ensure we're using the new version\n",
        "import importlib\n",
        "import sys\n",
        "if 'gradio' in sys.modules:\n",
        "    del sys.modules['gradio']  # Force complete reload\n",
        "\n",
        "# Try importing and check version\n",
        "try:\n",
        "    import gradio\n",
        "    print(f\"‚úÖ Successfully installed Gradio version: {gradio.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error importing Gradio: {e}\")\n",
        "    print(\"   You may need to restart the runtime (Runtime > Restart runtime)\")\n",
        "\n",
        "# Verify other key dependencies are still working\n",
        "print(\"\\n--- Checking core dependencies ---\")\n",
        "libs_to_check = [\"torch\", \"diffusers\", \"transformers\", \"numpy\"]\n",
        "for lib_name in libs_to_check:\n",
        "    try:\n",
        "        lib = importlib.import_module(lib_name)\n",
        "        version = getattr(lib, '__version__', \"unknown\")\n",
        "        print(f\"‚úÖ {lib_name}: {version}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {lib_name} error: {e}\")\n",
        "\n",
        "print(\"\\nIf all dependencies show ‚úÖ, you're ready to proceed with the notebook.\")\n",
        "print(\"If there are still errors, try running: Runtime > Restart runtime, then run all cells from the beginning.\")"
      ],
      "metadata": {
        "id": "-ufzN0dxjxjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Environment Setup - Dependencies (Attempt 10 - Force Reinstall NumPy/Diffusers)\n",
        "\n",
        "# --- Upgrade pip ---\n",
        "print(\"Upgrading pip...\")\n",
        "!pip install --upgrade pip\n",
        "print(\"Pip upgrade complete.\")\n",
        "\n",
        "# --- Install PyTorch ---\n",
        "print(\"Installing PyTorch 2.1.0...\")\n",
        "# Sticking with 2.1.0 as it seemed okay before the numpy issue\n",
        "!pip install -q torch==2.1.0+cu121 torchvision==0.16.0+cu121 torchaudio==2.1.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# --- Install Core Libs ---\n",
        "print(\"Installing core libraries (forcing reinstall for numpy and diffusers)...\")\n",
        "\n",
        "# Force reinstall numpy and diffusers, keep others as specified\n",
        "# Using -q for quiet, but errors should still show. Remove -q if detailed logs needed.\n",
        "!pip install -q \\\n",
        "    --force-reinstall numpy==1.24.3 \\\n",
        "    Pillow==10.4.0 \\\n",
        "    gradio==3.41.2 \\\n",
        "    huggingface_hub==0.23.3 \\\n",
        "    transformers==4.42.4 \\\n",
        "    accelerate==0.32.1 \\\n",
        "    --force-reinstall diffusers==0.25.1 \\\n",
        "    torchsde==0.2.6 \\\n",
        "    einops==0.8.0 \\\n",
        "    safetensors==0.4.3 \\\n",
        "    pyyaml==6.0.1 \\\n",
        "    scipy==1.14.0 \\\n",
        "    tqdm==4.66.4 \\\n",
        "    psutil==6.0.0 \\\n",
        "    pytorch_lightning==2.3.3 \\\n",
        "    omegaconf==2.3.0 \\\n",
        "    pygit2==1.15.1 \\\n",
        "    opencv-contrib-python-headless==4.10.0.84 \\\n",
        "    httpx==0.27.0 \\\n",
        "    onnxruntime==1.18.1 \\\n",
        "    timm==1.0.7 \\\n",
        "    tokenizers==0.19.1 \\\n",
        "    packaging==24.1 \\\n",
        "    piexif \\\n",
        "    sentencepiece \\\n",
        "    requests\n",
        "\n",
        "# --- Excluded Dependencies ---\n",
        "# insightface, onnxruntime-gpu (for Woop - deferred)\n",
        "# segment-anything-hq, supervision (for SAM - removed)\n",
        "# invisible_watermark, etc. (Keeping it minimal)\n",
        "\n",
        "# --- Clear Output & Verify ---\n",
        "import IPython\n",
        "import importlib\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import numpy # Import numpy again after reinstall for verification\n",
        "\n",
        "# Comment out clear_output to see full install log if needed\n",
        "# IPython.display.clear_output()\n",
        "\n",
        "print(\"\\n--- Dependency Installation Summary (Attempt 10 - Forced Reinstall) ---\")\n",
        "# Check libraries needed for this simplified plan\n",
        "libs_to_check = {\n",
        "    \"numpy\": \"numpy\",\n",
        "    \"torch\": \"torch\",\n",
        "    \"PIL\": \"PIL\", # Pillow\n",
        "    \"huggingface_hub\": \"huggingface_hub\",\n",
        "    \"diffusers\": \"diffusers\",\n",
        "    \"transformers\": \"transformers\",\n",
        "    \"accelerate\": \"accelerate\",\n",
        "    \"gradio\": \"gradio\",\n",
        "    \"safetensors\": \"safetensors\",\n",
        "    \"cv2\": \"cv2\",\n",
        "    \"onnxruntime\": \"onnxruntime\",\n",
        "    \"requests\": \"requests\",\n",
        "}\n",
        "error_found = False\n",
        "successful_imports = []\n",
        "failed_imports = []\n",
        "\n",
        "# Check imports carefully\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"Running import checks...\")\n",
        "for display_name, lib_name in libs_to_check.items():\n",
        "    try:\n",
        "        # Use reload if module already imported (like numpy above)\n",
        "        if lib_name in sys.modules:\n",
        "             mod = importlib.reload(sys.modules[lib_name])\n",
        "        else:\n",
        "             mod = importlib.import_module(lib_name)\n",
        "\n",
        "        version = \"N/A\"\n",
        "        if hasattr(mod, '__version__'):\n",
        "            version = mod.__version__\n",
        "        elif lib_name == \"PIL\":\n",
        "             try:\n",
        "                 import Pillow\n",
        "                 version = Pillow.__version__\n",
        "             except ImportError: pass\n",
        "             except AttributeError: pass\n",
        "\n",
        "        # --- Specific NumPy dtypes check ---\n",
        "        if lib_name == \"numpy\":\n",
        "             has_dtypes_check = hasattr(mod, 'dtypes')\n",
        "             print(f\"‚úÖ {display_name}: {version} (dtypes exists: {has_dtypes_check})\")\n",
        "             if not has_dtypes_check:\n",
        "                  print(f\"   üö® CRITICAL WARNING: NumPy {version} still missing 'dtypes' after reinstall!\")\n",
        "                  error_found = True # Treat this as an error\n",
        "        else:\n",
        "             print(f\"‚úÖ {display_name}: {version}\")\n",
        "\n",
        "        successful_imports.append(display_name)\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ùå {display_name} not found. Error: {e}\")\n",
        "        failed_imports.append(display_name)\n",
        "        error_found = True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error importing/checking {display_name}. Error: {e}\")\n",
        "        failed_imports.append(display_name)\n",
        "        error_found = True\n",
        "\n",
        "\n",
        "print(\"\\n--- GPU Check ---\")\n",
        "# Check torch import status from the loop above\n",
        "torch_failed = any(item == \"torch\" for item in failed_imports)\n",
        "\n",
        "if torch_failed:\n",
        "     print(\"‚ùå Torch import failed, cannot check GPU.\")\n",
        "     error_found = True # Ensure error is flagged\n",
        "elif torch.cuda.is_available():\n",
        "    try:\n",
        "        print(f\"‚úÖ GPU Found: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   CUDA Version Used by PyTorch: {torch.version.cuda}\")\n",
        "        print(f\"   PyTorch Version: {torch.__version__}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during GPU check: {e}\")\n",
        "        error_found = True\n",
        "else:\n",
        "    print(\"‚ùå No GPU detected. This notebook requires a GPU runtime.\")\n",
        "    error_found = True\n",
        "\n",
        "print(\"\\n--- Summary ---\")\n",
        "if error_found:\n",
        "     print(f\"‚ö†Ô∏è Errors encountered during setup or import checks. Failed/Problematic imports: {failed_imports}\")\n",
        "     print(f\"   If NumPy still shows 'dtypes exists: False', the environment conflict persists.\")\n",
        "     print(f\"   You might need to try different versions of libraries (diffusers, torch, numpy) or factory reset the runtime.\")\n",
        "else:\n",
        "     print(\"‚úÖ Core environment setup cell completed successfully (Attempt 10 - Forced Reinstall).\")\n",
        "     print(f\"   Successfully imported: {successful_imports}\")\n",
        "     print(f\"   NumPy dtypes check passed.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NDl5YPB2w-VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Google Drive Integration\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üîÑ Mounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True) # Force remount can help avoid errors if already mounted\n",
        "    print(\"‚úÖ Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error mounting Google Drive: {e}\")\n",
        "    raise Exception(\"Drive mounting failed. Please check permissions and try again.\")\n",
        "\n",
        "# --- Define Directory Structure ---\n",
        "# Using a clear base path in MyDrive\n",
        "BASE_DRIVE_PATH = '/content/drive/MyDrive/AI_Studio_Toolkit_v3' # Changed name for v3\n",
        "MODELS_PATH = os.path.join(BASE_DRIVE_PATH, 'models')\n",
        "SAM_MODELS_PATH = os.path.join(MODELS_PATH, 'sam_models') # Specific path for SAM models\n",
        "WOOP_MODELS_PATH = os.path.join(MODELS_PATH, 'woop_models') # Specific path for Woop models\n",
        "OUTPUT_PATH = os.path.join(BASE_DRIVE_PATH, 'outputs')\n",
        "CONFIG_PATH = os.path.join(BASE_DRIVE_PATH, 'config') # For storing config files like tokens\n",
        "\n",
        "# List of directories to create\n",
        "paths_to_create = [\n",
        "    BASE_DRIVE_PATH,\n",
        "    MODELS_PATH,\n",
        "    SAM_MODELS_PATH,\n",
        "    WOOP_MODELS_PATH,\n",
        "    OUTPUT_PATH,\n",
        "    CONFIG_PATH\n",
        "]\n",
        "\n",
        "print(\"\\n--- Creating Project Directories ---\")\n",
        "for path in paths_to_create:\n",
        "    try:\n",
        "        os.makedirs(path, exist_ok=True) # exist_ok=True prevents error if dir already exists\n",
        "        print(f\"‚úì Directory ensured: {path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"‚ùå Error creating directory {path}: {e}\")\n",
        "        # Decide if this error is critical. For now, we print and continue.\n",
        "        # Depending on the error, you might want to raise it.\n",
        "\n",
        "# --- Verify Write Permissions ---\n",
        "print(\"\\n--- Verifying Drive Write Access ---\")\n",
        "test_file_path = os.path.join(BASE_DRIVE_PATH, '.write_test')\n",
        "try:\n",
        "    with open(test_file_path, 'w') as f:\n",
        "        f.write(datetime.now().isoformat())\n",
        "    os.remove(test_file_path) # Clean up the test file\n",
        "    print(\"‚úÖ Google Drive is writable.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Write access test failed in {BASE_DRIVE_PATH}: {e}\")\n",
        "    print(\"   Please ensure Colab has write permissions for your Google Drive.\")\n",
        "    # Consider raising an exception if write access is absolutely critical here\n",
        "    # raise Exception(\"Write access to Google Drive failed.\")\n",
        "\n",
        "# --- Define Key File Paths (for later use) ---\n",
        "TOKENS_FILE = os.path.join(CONFIG_PATH, 'tokens.json')\n",
        "\n",
        "print(f\"\\n‚úÖ Google Drive setup complete. Project base path: {BASE_DRIVE_PATH}\")"
      ],
      "metadata": {
        "id": "jxuL9xPauld3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Configuration & Token Management (with Colab Forms)\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.colab import output # Used for managing Colab Forms output\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure BASE_DRIVE_PATH and CONFIG_PATH are defined from Cell 3\n",
        "# If running this cell independently, uncomment and define them:\n",
        "# BASE_DRIVE_PATH = '/content/drive/MyDrive/AI_Studio_Toolkit_v3'\n",
        "# CONFIG_PATH = os.path.join(BASE_DRIVE_PATH, 'config')\n",
        "TOKENS_FILE = os.path.join(CONFIG_PATH, 'tokens.json')\n",
        "\n",
        "# --- TokenManager Class ---\n",
        "class TokenManager:\n",
        "    \"\"\"Handles loading and saving API tokens securely.\"\"\"\n",
        "\n",
        "    def __init__(self, filepath):\n",
        "        \"\"\"\n",
        "        Initializes the TokenManager.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): The path to the JSON file where tokens are stored.\n",
        "        \"\"\"\n",
        "        self.filepath = filepath\n",
        "        # Ensure the config directory exists (it should from Cell 3)\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "    def load_tokens(self):\n",
        "        \"\"\"Loads tokens from the JSON file.\"\"\"\n",
        "        if os.path.exists(self.filepath):\n",
        "            try:\n",
        "                with open(self.filepath, 'r') as f:\n",
        "                    tokens = json.load(f)\n",
        "                    print(f\"üîë Tokens loaded from: {self.filepath}\")\n",
        "                    return tokens\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"‚ö†Ô∏è Warning: Token file '{self.filepath}' is corrupted. Starting fresh.\")\n",
        "                return {}\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Warning: Could not read token file '{self.filepath}'. Error: {e}\")\n",
        "                return {}\n",
        "        else:\n",
        "            print(f\"‚ÑπÔ∏è Token file not found at: {self.filepath}. Will create if tokens are saved.\")\n",
        "            return {} # Return empty dict if file doesn't exist\n",
        "\n",
        "    def save_tokens(self, tokens):\n",
        "        \"\"\"\n",
        "        Saves the provided tokens to the JSON file.\n",
        "\n",
        "        Args:\n",
        "            tokens (dict): A dictionary containing the tokens to save.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(self.filepath, 'w') as f:\n",
        "                json.dump(tokens, f, indent=4) # Use indent for readability\n",
        "            print(f\"üíæ Tokens successfully saved to: {self.filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error saving tokens to {self.filepath}: {e}\")\n",
        "\n",
        "# --- Initialize Token Manager & Load Existing Tokens ---\n",
        "token_manager = TokenManager(TOKENS_FILE)\n",
        "existing_tokens = token_manager.load_tokens()\n",
        "\n",
        "# --- Colab Forms for Token Input ---\n",
        "# Clear previous form output to avoid clutter\n",
        "output.clear()\n",
        "\n",
        "print(\"--- API Token Configuration ---\")\n",
        "print(\"Enter your API tokens below. They are required for downloading models.\")\n",
        "print(\"Tokens are saved to your Google Drive and are *not* printed in the output.\")\n",
        "print(\"‚ùó Important: Do not share your notebook with tokens saved if you are concerned about security.\")\n",
        "\n",
        "# Prepare descriptions for the form, showing if a token is already saved\n",
        "hf_token_status = \"(Optional, Recommended) - Found in saved file.\" if existing_tokens.get('hf_token') else \"(Optional, Recommended)\"\n",
        "civitai_token_status = \"(Optional) - Found in saved file.\" if existing_tokens.get('civitai_token') else \"(Optional)\"\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Hugging Face Token\n",
        "#@markdown Get yours from: https://huggingface.co/settings/tokens\n",
        "hf_token_input = \"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\" #@param {type:\"string\"}\n",
        "#@markdown **Save Hugging Face Token to Google Drive?** (Overwrites existing if checked)\n",
        "save_hf_token = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Civitai API Key\n",
        "#@markdown Get yours from: https://civitai.com/user/account (Create API Key section)\n",
        "civitai_token_input = \"8cd1ea2f1f643d6ce1e04c2d5dea119b\" #@param {type:\"string\"}\n",
        "#@markdown **Save Civitai API Key to Google Drive?** (Overwrites existing if checked)\n",
        "save_civitai_token = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- Process and Save Tokens ---\n",
        "tokens_to_save = existing_tokens.copy() # Start with existing tokens\n",
        "\n",
        "# Update Hugging Face token if provided and save is checked\n",
        "if hf_token_input and save_hf_token:\n",
        "    tokens_to_save['hf_token'] = hf_token_input.strip()\n",
        "    print(\"‚ÑπÔ∏è Hugging Face token provided and marked for saving.\")\n",
        "elif hf_token_input and not save_hf_token:\n",
        "     print(\"‚ÑπÔ∏è Hugging Face token provided but *not* marked for saving to Drive.\")\n",
        "     # Optionally, you could store it temporarily in a variable for the session\n",
        "     # session_hf_token = hf_token_input.strip()\n",
        "elif not hf_token_input and save_hf_token:\n",
        "     print(\"‚ö†Ô∏è Warning: 'Save Hugging Face Token' checked, but no token was entered.\")\n",
        "\n",
        "# Update Civitai token if provided and save is checked\n",
        "if civitai_token_input and save_civitai_token:\n",
        "    tokens_to_save['civitai_token'] = civitai_token_input.strip()\n",
        "    print(\"‚ÑπÔ∏è Civitai token provided and marked for saving.\")\n",
        "elif civitai_token_input and not save_civitai_token:\n",
        "     print(\"‚ÑπÔ∏è Civitai token provided but *not* marked for saving to Drive.\")\n",
        "     # session_civitai_token = civitai_token_input.strip()\n",
        "elif not civitai_token_input and save_civitai_token:\n",
        "     print(\"‚ö†Ô∏è Warning: 'Save Civitai API Key' checked, but no key was entered.\")\n",
        "\n",
        "\n",
        "# Save if any changes were marked for saving\n",
        "if save_hf_token or save_civitai_token:\n",
        "    # Only save if at least one relevant token was actually provided\n",
        "    if (save_hf_token and hf_token_input) or (save_civitai_token and civitai_token_input):\n",
        "         token_manager.save_tokens(tokens_to_save)\n",
        "    else:\n",
        "         print(\"‚ÑπÔ∏è No new tokens were provided to save, even though a save box was checked.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No tokens were marked for saving to Google Drive in this run.\")\n",
        "\n",
        "\n",
        "# Display final status (partially masked for security)\n",
        "print(\"\\n--- Current Token Status ---\")\n",
        "loaded_hf = tokens_to_save.get('hf_token')\n",
        "loaded_civitai = tokens_to_save.get('civitai_token')\n",
        "\n",
        "if loaded_hf:\n",
        "    print(f\"üîë Hugging Face Token: Loaded (hf_...{loaded_hf[-4:]})\")\n",
        "else:\n",
        "    print(\"‚ùì Hugging Face Token: Not configured.\")\n",
        "\n",
        "if loaded_civitai:\n",
        "     # Civitai keys can be shorter, adjust masking if needed\n",
        "     mask_len = min(4, len(loaded_civitai) // 2)\n",
        "     print(f\"üîë Civitai API Key: Loaded (...{loaded_civitai[-mask_len:]})\")\n",
        "else:\n",
        "    print(\"‚ùì Civitai API Key: Not configured.\")\n",
        "\n",
        "print(\"\\n‚úÖ Token configuration cell complete.\")\n"
      ],
      "metadata": {
        "id": "EmLSfMstlyBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Model Management (with Pre-Download Form)\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import shutil\n",
        "import json\n",
        "from tqdm.notebook import tqdm # For progress bars\n",
        "from huggingface_hub import snapshot_download, HfApi, hf_hub_url, hf_hub_download\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import time\n",
        "from google.colab import output # To potentially clear form output if needed\n",
        "\n",
        "# --- Configuration Flags ---\n",
        "# Set these flags to True to enable the corresponding functionality\n",
        "# NOTE: Enabling these also requires installing the necessary dependencies (e.g., in Cell 2)\n",
        "ENABLE_SAM = False\n",
        "ENABLE_WOOP = False\n",
        "\n",
        "# --- Configuration & Paths ---\n",
        "# Assuming variables from Cell 3 & 4 are available:\n",
        "# BASE_DRIVE_PATH, MODELS_PATH, SAM_MODELS_PATH, WOOP_MODELS_PATH, CONFIG_PATH, TOKENS_FILE\n",
        "# Also assuming 'tokens_to_save' dictionary from Cell 4 is available globally.\n",
        "\n",
        "# --- Standalone Setup (for independent execution, keep commented otherwise) ---\n",
        "# BASE_DRIVE_PATH = '/content/drive/MyDrive/AI_Studio_Toolkit_v3'\n",
        "# MODELS_PATH = os.path.join(BASE_DRIVE_PATH, 'models')\n",
        "# SAM_MODELS_PATH = os.path.join(MODELS_PATH, 'sam_models') # Path defined even if disabled\n",
        "# WOOP_MODELS_PATH = os.path.join(MODELS_PATH, 'woop_models') # Path defined even if disabled\n",
        "# CONFIG_PATH = os.path.join(BASE_DRIVE_PATH, 'config')\n",
        "# TOKENS_FILE = os.path.join(CONFIG_PATH, 'tokens.json')\n",
        "# class StandaloneTokenManager:\n",
        "#     def __init__(self, filepath): self.filepath = filepath\n",
        "#     def load_tokens(self):\n",
        "#         if os.path.exists(self.filepath):\n",
        "#             try:\n",
        "#                 with open(self.filepath, 'r') as f: return json.load(f)\n",
        "#             except: return {}\n",
        "#         return {}\n",
        "# token_manager_standalone = StandaloneTokenManager(TOKENS_FILE)\n",
        "# tokens_to_save = token_manager_standalone.load_tokens()\n",
        "# print(f\"[Standalone] Loaded tokens: {tokens_to_save}\")\n",
        "# --- End Standalone Setup ---\n",
        "\n",
        "\n",
        "print(\"--- Initializing Model Manager ---\")\n",
        "print(f\"SAM Functionality Enabled: {ENABLE_SAM}\")\n",
        "print(f\"Woop Functionality Enabled: {ENABLE_WOOP}\")\n",
        "\n",
        "\n",
        "# --- Predefined Model List ---\n",
        "# Format: 'Display Name': ('type', 'identifier', 'optional_subfolder', 'optional_filename_filter')\n",
        "PREDEFINED_MODELS = {\n",
        "    # --- Hugging Face Models ---\n",
        "    \"Deliberate V3 (HF)\": ('hf', 'stablediffusionapi/deliberate-v3', 'deliberate-v3', None),\n",
        "    \"Anything V5 (HF)\": ('hf', 'stablediffusionapi/anything-v5', 'anything-v5', None),\n",
        "    \"Realistic Vision V5.1 (HF)\": ('hf', 'SG161222/Realistic_Vision_V5.1_noVAE', 'Realistic_Vision_V5.1_noVAE', None),\n",
        "    \"DreamShaper 8 (HF)\": ('hf', 'Lykon/dreamshaper-8', 'dreamshaper-8', None),\n",
        "    \"Absolute Reality V1.8.1 (HF)\": ('hf', 'stablediffusionapi/absolute-reality-v1.8.1', 'absolute-reality-v1.8.1', None),\n",
        "    # --- Civitai Models ---\n",
        "    \"ChilloutMix (Civitai)\": ('civitai', '11745', 'chilloutmix', '.safetensors'),\n",
        "    \"OrangeMixs (Civitai)\": ('civitai', '49829', 'OrangeMixs', '.safetensors'),\n",
        "    \"Perfect Deliberate (Civitai)\": ('civitai', '128456', 'perfectdeliberate-v5', '.safetensors'),\n",
        "    # --- SAM Models (Only active if ENABLE_SAM=True) ---\n",
        "    \"SAM ViT-H (Default)\": ('sam', 'sam_vit_h_4b8939.pth', 'sam_vit_h', None), # Added subfolder\n",
        "    # --- Woop Models (Only active if ENABLE_WOOP=True) ---\n",
        "    \"Woop Insighter FaceAnalysis\": ('woop', 'inswapper_128.onnx', 'inswapper', None), # Added subfolder\n",
        "}\n",
        "\n",
        "# --- ModelManager Class ---\n",
        "class ModelManager:\n",
        "    \"\"\"Handles downloading, organizing, and listing models, with conditional SAM/Woop.\"\"\"\n",
        "\n",
        "    def __init__(self, models_base_path, sam_models_path, woop_models_path, tokens):\n",
        "        self.models_base_path = models_base_path\n",
        "        self.sam_models_path = sam_models_path # Store path even if disabled\n",
        "        self.woop_models_path = woop_models_path # Store path even if disabled\n",
        "        self.tokens = tokens\n",
        "        self.hf_token = self.tokens.get('hf_token')\n",
        "        self.civitai_token = self.tokens.get('civitai_token')\n",
        "        self.hf_api = HfApi(token=self.hf_token) if self.hf_token else HfApi()\n",
        "\n",
        "        # Ensure base diffusion models directory exists\n",
        "        os.makedirs(self.models_base_path, exist_ok=True)\n",
        "        print(f\"‚úì Diffusion Model Base Path: {self.models_base_path}\")\n",
        "\n",
        "        # Conditionally ensure SAM/Woop directories exist\n",
        "        if ENABLE_SAM:\n",
        "            os.makedirs(self.sam_models_path, exist_ok=True)\n",
        "            print(f\"‚úì SAM Models Path (Enabled): {self.sam_models_path}\")\n",
        "        else:\n",
        "            print(f\"‚ÑπÔ∏è SAM Models Path (Disabled): {self.sam_models_path}\")\n",
        "\n",
        "        if ENABLE_WOOP:\n",
        "            os.makedirs(self.woop_models_path, exist_ok=True)\n",
        "            print(f\"‚úì Woop Models Path (Enabled): {self.woop_models_path}\")\n",
        "        else:\n",
        "            print(f\"‚ÑπÔ∏è Woop Models Path (Disabled): {self.woop_models_path}\")\n",
        "\n",
        "\n",
        "    def _get_target_dir(self, model_type, identifier, subfolder_name=None):\n",
        "        \"\"\"Determines the correct target directory based on model type.\"\"\"\n",
        "        # Helper to create safe directory names\n",
        "        def safe_name(name):\n",
        "            # Basic sanitization, replace slashes and backslashes\n",
        "            return name.replace('/', '_').replace('\\\\', '_')\n",
        "\n",
        "        if model_type in ['hf', 'civitai']:\n",
        "            base = self.models_base_path\n",
        "            # Use provided subfolder or generate one from identifier\n",
        "            effective_subfolder = subfolder_name if subfolder_name else safe_name(identifier)\n",
        "            return os.path.join(base, effective_subfolder)\n",
        "\n",
        "        elif model_type == 'sam':\n",
        "            if not ENABLE_SAM: return None # Return None if SAM is disabled\n",
        "            base = self.sam_models_path\n",
        "            effective_subfolder = subfolder_name if subfolder_name else safe_name(identifier)\n",
        "            return os.path.join(base, effective_subfolder)\n",
        "\n",
        "        elif model_type == 'woop':\n",
        "            if not ENABLE_WOOP: return None # Return None if Woop is disabled\n",
        "            base = self.woop_models_path\n",
        "            effective_subfolder = subfolder_name if subfolder_name else safe_name(identifier)\n",
        "            return os.path.join(base, effective_subfolder)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Unknown model type '{model_type}'. Cannot determine target directory.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def list_local_models(self, model_type='all'):\n",
        "        \"\"\"Lists downloaded models in the specified directories, respecting enabled features.\"\"\"\n",
        "        print(f\"\\n--- Listing Local Models (Type: {model_type}, SAM: {ENABLE_SAM}, Woop: {ENABLE_WOOP}) ---\")\n",
        "        found_models = {} # Dictionary to store 'DisplayName': 'path'\n",
        "\n",
        "        def find_models(path, type_label):\n",
        "            # Check if the path exists before trying to listdir\n",
        "            if not os.path.exists(path):\n",
        "                return {}\n",
        "            models = {}\n",
        "            try:\n",
        "                # Iterate through items (files or directories) in the path\n",
        "                for item in os.listdir(path):\n",
        "                    item_path = os.path.join(path, item)\n",
        "                    is_dir = os.path.isdir(item_path)\n",
        "                    # Check common model file extensions\n",
        "                    is_model_file = item.endswith(('.safetensors', '.ckpt', '.pth', '.onnx', '.bin'))\n",
        "\n",
        "                    if is_dir or is_model_file:\n",
        "                        found_name = item # Default to folder/file name\n",
        "                        # Try to map back to the predefined display name\n",
        "                        for name, details in PREDEFINED_MODELS.items():\n",
        "                            p_type, p_id, p_subfolder, p_filter = details\n",
        "                            # Match if the type matches AND...\n",
        "                            if p_type == type_label:\n",
        "                                # 1. Subfolder name matches the item name (common case)\n",
        "                                match_subfolder = p_subfolder == item\n",
        "                                # 2. Identifier matches the item name (e.g., for .pth, .onnx files)\n",
        "                                match_id_as_item = p_id == item\n",
        "                                # 3. Special check for HF repos where item is subfolder derived from repo_id\n",
        "                                match_hf_repo_subfolder = (p_type == 'hf' and item == p_id.replace('/', '_'))\n",
        "                                # 4. Check if item_path (for files) matches the expected file path based on get_model_path logic\n",
        "                                expected_file_path = None\n",
        "                                if is_model_file and p_type in ['civitai', 'sam', 'woop']:\n",
        "                                     # Reconstruct potential expected path (simplified check)\n",
        "                                     temp_target_dir = self._get_target_dir(p_type, p_id, p_subfolder)\n",
        "                                     if temp_target_dir:\n",
        "                                          # We don't know the exact filename downloaded by civitai easily here,\n",
        "                                          # so this check is less reliable for civitai without more state.\n",
        "                                          # Focus on matching subfolder or id for mapping back.\n",
        "                                          pass\n",
        "\n",
        "\n",
        "                                if match_subfolder or match_id_as_item or match_hf_repo_subfolder:\n",
        "                                    found_name = name\n",
        "                                    break\n",
        "                        # Store the display name -> path mapping\n",
        "                        # If a display name maps to multiple items (unlikely with good subfolders), last one wins\n",
        "                        models[found_name] = item_path\n",
        "            except Exception as e:\n",
        "                print(f\"Error listing models in {path}: {e}\")\n",
        "            return models\n",
        "\n",
        "        # List Diffusion models (HF/Civitai) - they share the base path\n",
        "        if model_type in ['hf', 'civitai', 'all']:\n",
        "            found_models.update(find_models(self.models_base_path, 'hf'))\n",
        "            found_models.update(find_models(self.models_base_path, 'civitai'))\n",
        "\n",
        "        # Conditionally list SAM models\n",
        "        if ENABLE_SAM and model_type in ['sam', 'all']:\n",
        "            found_models.update(find_models(self.sam_models_path, 'sam'))\n",
        "\n",
        "        # Conditionally list Woop models\n",
        "        if ENABLE_WOOP and model_type in ['woop', 'all']:\n",
        "            found_models.update(find_models(self.woop_models_path, 'woop'))\n",
        "\n",
        "        if not found_models:\n",
        "            print(\"No local models found matching the criteria and enabled features.\")\n",
        "        else:\n",
        "            print(\"Found:\")\n",
        "            # Sort by name for consistent output\n",
        "            for name in sorted(found_models.keys()):\n",
        "                 path = found_models[name]\n",
        "                 type_indicator = \"[Dir]\" if os.path.isdir(path) else \"[File]\"\n",
        "                 print(f\"  - {name} ({path}) {type_indicator}\")\n",
        "        return found_models\n",
        "\n",
        "\n",
        "    def get_model_path(self, model_name):\n",
        "        \"\"\"\n",
        "        Gets the local path of a model using its predefined name.\n",
        "        Returns the directory path for HF models, or the specific file path for others.\n",
        "        Returns None if not found or feature is disabled.\n",
        "        \"\"\"\n",
        "        if model_name not in PREDEFINED_MODELS:\n",
        "            return None\n",
        "\n",
        "        model_type, identifier, subfolder, filename_filter = PREDEFINED_MODELS[model_name]\n",
        "\n",
        "        # Check if the feature is enabled for SAM/Woop\n",
        "        if model_type == 'sam' and not ENABLE_SAM: return None\n",
        "        if model_type == 'woop' and not ENABLE_WOOP: return None\n",
        "\n",
        "        target_dir = self._get_target_dir(model_type, identifier, subfolder)\n",
        "        if not target_dir:\n",
        "             return None\n",
        "\n",
        "        # --- Hugging Face Model Check ---\n",
        "        if model_type == 'hf':\n",
        "            if os.path.isdir(target_dir):\n",
        "                # Check for indicator files\n",
        "                has_config = os.path.exists(os.path.join(target_dir, \"config.json\"))\n",
        "                has_model_index = os.path.exists(os.path.join(target_dir, \"model_index.json\"))\n",
        "                has_safetensors = any(f.endswith(\".safetensors\") for f in os.listdir(target_dir) if os.path.isfile(os.path.join(target_dir, f)))\n",
        "                if has_config or has_model_index or has_safetensors:\n",
        "                    return target_dir # Return directory path\n",
        "            return None # Directory doesn't exist or lacks key files\n",
        "\n",
        "        # --- Civitai, SAM, Woop Model Check (Expect single file within the subfolder) ---\n",
        "        elif model_type in ['civitai', 'sam', 'woop']:\n",
        "            if os.path.isdir(target_dir):\n",
        "                 expected_file = None\n",
        "                 try:\n",
        "                     files_in_dir = [f for f in os.listdir(target_dir) if os.path.isfile(os.path.join(target_dir, f))]\n",
        "                     # Try to find file matching the filter (Civitai) or identifier (SAM/Woop)\n",
        "                     for f in files_in_dir:\n",
        "                          matches_filter = filename_filter and filename_filter.lower() in f.lower()\n",
        "                          matches_id = identifier == f # SAM/Woop identifier is often the filename\n",
        "                          if matches_filter or (model_type in ['sam', 'woop'] and matches_id):\n",
        "                               expected_file = os.path.join(target_dir, f)\n",
        "                               break\n",
        "                     # Fallback: if only one relevant model file exists, assume it's the one\n",
        "                     if not expected_file:\n",
        "                          relevant_files = [f for f in files_in_dir if f.endswith(('.safetensors', '.ckpt', '.pth', '.onnx', '.bin'))]\n",
        "                          if len(relevant_files) == 1:\n",
        "                              expected_file = os.path.join(target_dir, relevant_files[0])\n",
        "\n",
        "                 except FileNotFoundError:\n",
        "                     return None # Directory doesn't exist\n",
        "\n",
        "                 if expected_file and os.path.exists(expected_file):\n",
        "                     return expected_file # Return specific file path\n",
        "            return None # Directory doesn't exist or expected file not found\n",
        "        else:\n",
        "            print(f\"‚ùå Unknown model type '{model_type}' for model '{model_name}'.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def download_model(self, model_name):\n",
        "        \"\"\"Downloads a model based on its predefined name, respecting enabled features.\"\"\"\n",
        "        if model_name not in PREDEFINED_MODELS:\n",
        "            print(f\"‚ùå Model '{model_name}' not found in predefined list.\")\n",
        "            return None\n",
        "\n",
        "        model_type, identifier, subfolder, filename_filter = PREDEFINED_MODELS[model_name]\n",
        "\n",
        "        # --- Feature Enablement Checks ---\n",
        "        if model_type == 'sam' and not ENABLE_SAM:\n",
        "            print(f\"‚ùå Cannot download '{model_name}': SAM functionality is disabled.\")\n",
        "            return None\n",
        "        if model_type == 'woop' and not ENABLE_WOOP:\n",
        "            print(f\"‚ùå Cannot download '{model_name}': Woop functionality is disabled.\")\n",
        "            return None\n",
        "\n",
        "        target_dir = self._get_target_dir(model_type, identifier, subfolder)\n",
        "        if not target_dir:\n",
        "             print(f\"‚ùå Could not determine target directory for '{model_name}'.\")\n",
        "             return None\n",
        "\n",
        "        # --- Check if Already Exists (before printing download message) ---\n",
        "        existing_path = self.get_model_path(model_name)\n",
        "        if existing_path:\n",
        "             print(f\"‚úÖ Model '{model_name}' already downloaded. Path: {existing_path}\")\n",
        "             return existing_path # Return existing path\n",
        "\n",
        "        # --- Proceed with Download ---\n",
        "        print(f\"\\n--- Downloading Model: {model_name} ---\")\n",
        "        print(f\"Type: {model_type}, Identifier: {identifier}, Target Dir: {target_dir}\")\n",
        "        os.makedirs(target_dir, exist_ok=True) # Ensure target dir exists\n",
        "\n",
        "        downloaded_path = None # Variable to store the final path of the downloaded item\n",
        "\n",
        "        try:\n",
        "            # --- Hugging Face Download ---\n",
        "            if model_type == 'hf':\n",
        "                print(f\"Downloading Hugging Face model repo '{identifier}'...\")\n",
        "                # snapshot_download returns the path to the downloaded directory\n",
        "                downloaded_path = snapshot_download(\n",
        "                    repo_id=identifier,\n",
        "                    local_dir=target_dir,\n",
        "                    token=self.hf_token,\n",
        "                    local_dir_use_symlinks=False,\n",
        "                    resume_download=True,\n",
        "                )\n",
        "                print(f\"‚úÖ Successfully downloaded HF model '{model_name}' to {downloaded_path}\")\n",
        "\n",
        "            # --- Civitai Download ---\n",
        "            elif model_type == 'civitai':\n",
        "                print(f\"Attempting to download Civitai model ID '{identifier}'...\")\n",
        "                if not self.civitai_token:\n",
        "                    print(\"‚ùå Civitai API Key required. Please configure in Cell 4.\")\n",
        "                    return None\n",
        "\n",
        "                # Fetch model version details\n",
        "                api_url = f\"https://civitai.com/api/v1/models/{identifier}\"\n",
        "                headers = {\"Authorization\": f\"Bearer {self.civitai_token}\"}\n",
        "                response = requests.get(api_url, headers=headers, timeout=30)\n",
        "                response.raise_for_status()\n",
        "                model_data = response.json()\n",
        "\n",
        "                download_url = None\n",
        "                selected_filename = None\n",
        "                # Find suitable file URL (logic unchanged from previous version)\n",
        "                if 'modelVersions' in model_data and model_data['modelVersions']:\n",
        "                    for version in model_data['modelVersions']:\n",
        "                         if 'files' in version and version['files']:\n",
        "                             primary_file = next((f for f in version['files'] if f.get('primary')), None)\n",
        "                             if primary_file and (not filename_filter or filename_filter.lower() in primary_file['name'].lower()):\n",
        "                                 download_url = primary_file['downloadUrl']\n",
        "                                 selected_filename = primary_file['name']\n",
        "                                 print(f\"Found primary file: {selected_filename} in version {version.get('name', 'N/A')}\")\n",
        "                                 break\n",
        "                             if not download_url:\n",
        "                                 for file_info in version['files']:\n",
        "                                     if filename_filter and filename_filter.lower() in file_info['name'].lower():\n",
        "                                         download_url = file_info['downloadUrl']\n",
        "                                         selected_filename = file_info['name']\n",
        "                                         print(f\"Found matching file: {selected_filename} in version {version.get('name', 'N/A')}\")\n",
        "                                         break\n",
        "                         if download_url: break\n",
        "\n",
        "                if not download_url or not selected_filename:\n",
        "                    print(f\"‚ùå Could not find suitable download URL for Civitai model ID {identifier} matching filter '{filename_filter}'.\")\n",
        "                    return None\n",
        "\n",
        "                # Download the file\n",
        "                final_download_url = f\"{download_url}?token={self.civitai_token}\"\n",
        "                filepath = os.path.join(target_dir, selected_filename)\n",
        "                print(f\"Downloading Civitai file: {selected_filename}\")\n",
        "                with requests.Session() as session:\n",
        "                     session.headers.update(headers)\n",
        "                     response = session.get(final_download_url, stream=True, timeout=120) # Longer timeout for large files\n",
        "                     response.raise_for_status()\n",
        "                     total_size = int(response.headers.get('content-length', 0))\n",
        "                     block_size = 1024 * 1024\n",
        "                     with open(filepath, 'wb') as f, tqdm(\n",
        "                         desc=selected_filename, total=total_size, unit='iB',\n",
        "                         unit_scale=True, unit_divisor=1024, leave=False # leave=False hides bar on completion\n",
        "                     ) as bar:\n",
        "                         for chunk in response.iter_content(chunk_size=block_size):\n",
        "                             if chunk:\n",
        "                                 size = f.write(chunk)\n",
        "                                 bar.update(size)\n",
        "                downloaded_path = filepath # Store the file path\n",
        "                print(f\"‚úÖ Successfully downloaded Civitai model '{model_name}' to {downloaded_path}\")\n",
        "\n",
        "\n",
        "            # --- SAM Download (Placeholder) ---\n",
        "            elif model_type == 'sam':\n",
        "                print(\"‚ÑπÔ∏è SAM model download - Attempting generic download...\")\n",
        "                SAM_BASE_URL = \"https://dl.fbaipublicfiles.com/segment_anything/\"\n",
        "                sam_url = f\"{SAM_BASE_URL}{identifier}\"\n",
        "                filepath = os.path.join(target_dir, identifier)\n",
        "                print(f\"Attempting download from: {sam_url}\")\n",
        "                try:\n",
        "                    response = requests.get(sam_url, stream=True, timeout=120)\n",
        "                    response.raise_for_status()\n",
        "                    total_size = int(response.headers.get('content-length', 0))\n",
        "                    block_size = 1024 * 1024\n",
        "                    with open(filepath, 'wb') as f, tqdm(\n",
        "                        desc=identifier, total=total_size, unit='iB',\n",
        "                        unit_scale=True, unit_divisor=1024, leave=False\n",
        "                    ) as bar:\n",
        "                        for chunk in response.iter_content(chunk_size=block_size):\n",
        "                            if chunk:\n",
        "                                size = f.write(chunk)\n",
        "                                bar.update(size)\n",
        "                    downloaded_path = filepath\n",
        "                    print(f\"‚úÖ Successfully downloaded SAM model '{model_name}' to {downloaded_path}\")\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                     print(f\"‚ùå Failed to download SAM model from {sam_url}. Error: {e}\")\n",
        "                     if os.path.exists(filepath): os.remove(filepath)\n",
        "                     return None # Explicitly return None on failure\n",
        "\n",
        "            # --- Woop Download (Placeholder) ---\n",
        "            elif model_type == 'woop':\n",
        "                print(\"‚ö†Ô∏è Woop (insightface) model download - Placeholder attempt...\")\n",
        "                woop_url = identifier # Assume identifier is URL\n",
        "                filepath = os.path.join(target_dir, os.path.basename(identifier))\n",
        "                print(f\"Attempting download from: {woop_url}\")\n",
        "                try:\n",
        "                    response = requests.get(woop_url, stream=True, timeout=60)\n",
        "                    response.raise_for_status()\n",
        "                    # Simplified download without progress for placeholder\n",
        "                    with open(filepath, 'wb') as f:\n",
        "                         for chunk in response.iter_content(chunk_size=8192):\n",
        "                             if chunk: f.write(chunk)\n",
        "                    downloaded_path = filepath\n",
        "                    print(f\"‚úÖ Placeholder download complete for Woop model '{model_name}' to {downloaded_path}\")\n",
        "                except Exception as e:\n",
        "                     print(f\"‚ùå Failed placeholder download for Woop model. Error: {e}\")\n",
        "                     return None # Explicitly return None on failure\n",
        "\n",
        "            else:\n",
        "                print(f\"‚ùå Unknown model type '{model_type}' for download.\")\n",
        "                return None\n",
        "\n",
        "            # Return the path of the downloaded item (directory for HF, file for others)\n",
        "            return downloaded_path\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"‚ùå Download failed for '{model_name}': Network error {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå An error occurred during download for '{model_name}': {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# --- Instantiate and Run ---\n",
        "print(\"\\nInstantiating ModelManager...\")\n",
        "model_manager = None # Initialize as None\n",
        "if 'tokens_to_save' not in globals():\n",
        "     print(\"‚ùå ERROR: 'tokens_to_save' dictionary not found. Please run Cell 4 first.\")\n",
        "else:\n",
        "     try:\n",
        "         # Pass the specific paths from Cell 3 context\n",
        "         model_manager = ModelManager(MODELS_PATH, SAM_MODELS_PATH, WOOP_MODELS_PATH, tokens_to_save)\n",
        "         print(\"\\n--- Initial Model Scan ---\")\n",
        "         model_manager.list_local_models() # List all models respecting enabled features\n",
        "     except Exception as e:\n",
        "         print(f\"‚ùå Failed to initialize ModelManager or scan models: {e}\")\n",
        "\n",
        "# --- Colab Form for Pre-Download ---\n",
        "if model_manager: # Only show form if manager initialized successfully\n",
        "    # Filter models for the dropdown based on enabled features\n",
        "    available_models_for_download = [\"(None - Skip Pre-Download)\"]\n",
        "    for name, details in PREDEFINED_MODELS.items():\n",
        "        model_type = details[0]\n",
        "        if model_type in ['hf', 'civitai']:\n",
        "            available_models_for_download.append(name)\n",
        "        elif model_type == 'sam' and ENABLE_SAM:\n",
        "            available_models_for_download.append(name)\n",
        "        elif model_type == 'woop' and ENABLE_WOOP:\n",
        "            available_models_for_download.append(name)\n",
        "\n",
        "    # Sort the list alphabetically, keeping \"(None...)\" at the top\n",
        "    available_models_for_download = [available_models_for_download[0]] + sorted(available_models_for_download[1:])\n",
        "\n",
        "    print(\"\\n--- Optional: Pre-Download a Model ---\")\n",
        "    print(\"Select a model to download now, ensuring it's ready when Gradio starts.\")\n",
        "    # Clear previous form output to prevent duplicates if re-run\n",
        "    output.clear(wait=True) # wait=True prevents flickering\n",
        "\n",
        "    #@markdown Select a model to download to Google Drive:\n",
        "    selected_model_to_download = \"Anything V5 (HF)\" #@param {type:\"string\"} [\"(None - Skip Pre-Download)\", \"Deliberate V3 (HF)\", \"Anything V5 (HF)\", \"Realistic Vision V5.1 (HF)\", \"DreamShaper 8 (HF)\", \"Absolute Reality V1.8.1 (HF)\", \"ChilloutMix (Civitai)\", \"OrangeMixs (Civitai)\", \"Perfect Deliberate (Civitai)\"]\n",
        "    # Note: Manually update the list in the @param line if PREDEFINED_MODELS changes significantly\n",
        "    # or regenerate this cell. For dynamic updates based on flags, manual update is easiest in Colab.\n",
        "\n",
        "    # --- Trigger Download Based on Form Selection ---\n",
        "    if selected_model_to_download != \"(None - Skip Pre-Download)\":\n",
        "        print(f\"\\nUser selected '{selected_model_to_download}' for pre-download.\")\n",
        "        if selected_model_to_download in PREDEFINED_MODELS:\n",
        "             # Call the download function - it handles checks for existing files\n",
        "             model_manager.download_model(selected_model_to_download)\n",
        "             print(\"\\n--- Model Scan After Pre-Download ---\") # Optional: Rescan after download\n",
        "             model_manager.list_local_models()\n",
        "        else:\n",
        "             print(f\"‚ö†Ô∏è Warning: Selected model '{selected_model_to_download}' not found in PREDEFINED_MODELS dictionary.\")\n",
        "    else:\n",
        "        print(\"\\n‚ÑπÔ∏è No model selected for pre-download.\")\n",
        "\n",
        "\n",
        "# --- Final Status ---\n",
        "if model_manager:\n",
        "    print(\"\\n‚úÖ Model Management cell setup complete.\")\n",
        "    print(f\"   SAM Enabled: {ENABLE_SAM}, Woop Enabled: {ENABLE_WOOP}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Model Management cell finished, but ModelManager could not be initialized (likely missing tokens or other error).\")\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sdwnGHM8nyrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Image Generation & Segmentation Pipelines (with NumPy Check)\n",
        "\n",
        "# --- NumPy Version Check ---\n",
        "# Add this block at the very beginning to diagnose the issue\n",
        "import sys\n",
        "print(f\"--- Checking NumPy Version ---\")\n",
        "try:\n",
        "    import numpy\n",
        "    print(f\"‚úÖ Found NumPy version: {numpy.__version__}\")\n",
        "    # Explicitly check for the problematic attribute\n",
        "    has_dtypes = hasattr(numpy, 'dtypes')\n",
        "    print(f\"   numpy.dtypes attribute exists: {has_dtypes}\")\n",
        "    if not has_dtypes:\n",
        "        print(f\"   ‚ö†Ô∏è The loaded NumPy ({numpy.__version__}) is missing the 'dtypes' attribute!\")\n",
        "        print(f\"      This is likely the cause of the error.\")\n",
        "        print(f\"      Expected version from Cell 2: 1.24.3\") # Adjust if Cell 2 changes\n",
        "        print(f\"      RECOMMENDATION: Restart Runtime and run all cells from the beginning.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"‚ùå NumPy not found. Please ensure it's installed correctly in Cell 2.\")\n",
        "    # Raise an error or exit if numpy is critical and missing\n",
        "    raise ImportError(\"NumPy is required but could not be imported.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An unexpected error occurred while checking NumPy: {e}\")\n",
        "print(f\"--- End NumPy Check ---\")\n",
        "# --- End NumPy Version Check ---\n",
        "\n",
        "\n",
        "import torch\n",
        "# import numpy as np # Already imported above for check\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from diffusers import (\n",
        "    DiffusionPipeline, StableDiffusionPipeline, StableDiffusionInpaintPipeline,\n",
        "    StableDiffusionImg2ImgPipeline, # Potentially useful for variations\n",
        "    DPMSolverMultistepScheduler, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler,\n",
        "    LMSDiscreteScheduler, DDIMScheduler, UniPCMultistepScheduler\n",
        ")\n",
        "import gc # Garbage collector for VRAM management\n",
        "from datetime import datetime\n",
        "import traceback # For detailed error printing if needed\n",
        "\n",
        "\n",
        "# --- Conditional SAM Imports ---\n",
        "# These imports will only succeed if SAM dependencies were installed (e.g., in Cell 2)\n",
        "# and ENABLE_SAM was set to True in Cell 5.\n",
        "if 'ENABLE_SAM' in globals() and ENABLE_SAM:\n",
        "    try:\n",
        "        # Use segment_anything_hq if available, otherwise fall back\n",
        "        try:\n",
        "            from segment_anything_hq import SamPredictor, sam_model_registry\n",
        "            print(\"‚úÖ Imported Segment Anything HQ\")\n",
        "        except ImportError:\n",
        "            try:\n",
        "                from segment_anything import SamPredictor, sam_model_registry\n",
        "                print(\"‚úÖ Imported Segment Anything (standard)\")\n",
        "            except ImportError:\n",
        "                print(\"‚ö†Ô∏è WARNING: ENABLE_SAM is True, but 'segment_anything' or 'segment_anything_hq' library not found.\")\n",
        "                print(\"   SAM functionality will be unavailable. Please install dependencies.\")\n",
        "                ENABLE_SAM = False # Disable SAM if import fails\n",
        "\n",
        "        # Supervision is often used for mask processing/visualization with SAM\n",
        "        try:\n",
        "            import supervision as sv\n",
        "            print(\"‚úÖ Imported Supervision\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è Warning: 'supervision' library not found. Mask processing helpers might be limited.\")\n",
        "\n",
        "        # OpenCV is usually required by SAM/Supervision\n",
        "        try:\n",
        "            import cv2\n",
        "            print(\"‚úÖ Imported OpenCV (cv2)\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è WARNING: ENABLE_SAM is True, but 'opencv-python' library not found.\")\n",
        "            print(\"   SAM functionality might be impaired. Please install dependencies.\")\n",
        "            # Consider disabling SAM if cv2 is strictly required by the implementation\n",
        "            # ENABLE_SAM = False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during conditional SAM/related imports: {e}\")\n",
        "        ENABLE_SAM = False # Disable SAM on unexpected import errors\n",
        "else:\n",
        "    # Ensure ENABLE_SAM is False if it wasn't defined earlier (e.g., running cell standalone)\n",
        "    if 'ENABLE_SAM' not in globals():\n",
        "        ENABLE_SAM = False\n",
        "    print(\"‚ÑπÔ∏è SAM functionality is disabled (ENABLE_SAM=False or flag not found).\")\n",
        "\n",
        "\n",
        "# --- Constants and Configuration ---\n",
        "DEFAULT_SCHEDULER = \"DPMSolverMultistepScheduler\" # Default sampler\n",
        "AVAILABLE_SCHEDULERS = {\n",
        "    \"DPMSolverMultistepScheduler\": DPMSolverMultistepScheduler,\n",
        "    \"EulerDiscreteScheduler\": EulerDiscreteScheduler,\n",
        "    \"EulerAncestralDiscreteScheduler\": EulerAncestralDiscreteScheduler,\n",
        "    \"UniPCMultistepScheduler\": UniPCMultistepScheduler,\n",
        "    \"LMSDiscreteScheduler\": LMSDiscreteScheduler,\n",
        "    \"DDIMScheduler\": DDIMScheduler,\n",
        "}\n",
        "# Default path for SAM model if enabled (ensure this matches PREDEFINED_MODELS in Cell 5)\n",
        "DEFAULT_SAM_MODEL_NAME = \"SAM ViT-H (Default)\"\n",
        "\n",
        "\n",
        "# --- Image Generation Class ---\n",
        "class ImageGenerator:\n",
        "    \"\"\"Handles loading models and generating images using various pipelines.\"\"\"\n",
        "\n",
        "    def __init__(self, model_manager, output_path):\n",
        "        \"\"\"\n",
        "        Initializes the ImageGenerator.\n",
        "\n",
        "        Args:\n",
        "            model_manager (ModelManager): Instance of the ModelManager from Cell 5.\n",
        "            output_path (str): Path to the directory where generated images will be saved.\n",
        "        \"\"\"\n",
        "        self.model_manager = model_manager\n",
        "        self.output_path = output_path\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"ImageGenerator initialized on device: {self.device}\")\n",
        "        if self.device == \"cpu\":\n",
        "            print(\"‚ö†Ô∏è Warning: No GPU detected. Image generation will be very slow.\")\n",
        "\n",
        "        self.current_pipeline = None\n",
        "        self.current_model_name = None # Store the display name of the loaded model\n",
        "        self.current_pipeline_type = None # 'txt2img', 'inpaint', etc.\n",
        "\n",
        "        # --- Conditional SAM Initialization ---\n",
        "        self.sam_predictor = None\n",
        "        self.sam_model_name = None\n",
        "        # Use globals().get() for safer check in case ENABLE_SAM wasn't defined somehow\n",
        "        if globals().get('ENABLE_SAM', False):\n",
        "            self._initialize_sam()\n",
        "\n",
        "    def _initialize_sam(self):\n",
        "        \"\"\"Loads the SAM model if enabled and available.\"\"\"\n",
        "        # Double-check flag just in case\n",
        "        if not globals().get('ENABLE_SAM', False):\n",
        "            print(\"‚ÑπÔ∏è SAM is disabled, skipping initialization.\")\n",
        "            return\n",
        "        if self.sam_predictor:\n",
        "             print(\"‚ÑπÔ∏è SAM predictor already initialized.\")\n",
        "             return\n",
        "\n",
        "        print(\"--- Initializing SAM Predictor ---\")\n",
        "        # Ensure model_manager is valid\n",
        "        if not hasattr(self, 'model_manager') or self.model_manager is None:\n",
        "             print(\"‚ùå Cannot initialize SAM: ModelManager is not available.\")\n",
        "             globals()['ENABLE_SAM'] = False\n",
        "             print(\"‚ö†Ô∏è SAM functionality has been disabled.\")\n",
        "             return\n",
        "\n",
        "        sam_model_path = self.model_manager.get_model_path(DEFAULT_SAM_MODEL_NAME)\n",
        "\n",
        "        if not sam_model_path or not os.path.exists(sam_model_path):\n",
        "            print(f\"‚ùå SAM model file not found at expected path derived from '{DEFAULT_SAM_MODEL_NAME}'. Looked for: {sam_model_path}\")\n",
        "            print(f\"   Please download it using the Model Manager (Cell 5).\")\n",
        "            globals()['ENABLE_SAM'] = False # Update global flag\n",
        "            print(\"‚ö†Ô∏è SAM functionality has been disabled due to missing model.\")\n",
        "            return\n",
        "\n",
        "        # Determine SAM model type from filename (common convention)\n",
        "        sam_filename = os.path.basename(sam_model_path)\n",
        "        if \"vit_h\" in sam_filename: model_type = \"vit_h\"\n",
        "        elif \"vit_l\" in sam_filename: model_type = \"vit_l\"\n",
        "        elif \"vit_b\" in sam_filename: model_type = \"vit_b\"\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Could not determine SAM model type from filename: {sam_filename}. Assuming 'vit_h'.\")\n",
        "            model_type = \"vit_h\" # Default guess\n",
        "\n",
        "        try:\n",
        "            print(f\"Loading SAM model (type: {model_type}) from: {sam_model_path}\")\n",
        "            # Ensure sam_model_registry is available\n",
        "            if 'sam_model_registry' not in globals():\n",
        "                 raise NameError(\"sam_model_registry not found. SAM library import likely failed.\")\n",
        "\n",
        "            sam = sam_model_registry[model_type](checkpoint=sam_model_path)\n",
        "            sam.to(device=self.device)\n",
        "            self.sam_predictor = SamPredictor(sam)\n",
        "            self.sam_model_name = DEFAULT_SAM_MODEL_NAME # Store name of loaded SAM model\n",
        "            print(f\"‚úÖ SAM predictor initialized successfully with '{self.sam_model_name}'.\")\n",
        "\n",
        "        except NameError as ne:\n",
        "             print(f\"‚ùå Error initializing SAM: {ne}. Library might not be imported correctly.\")\n",
        "             globals()['ENABLE_SAM'] = False\n",
        "             print(\"‚ö†Ô∏è SAM functionality has been disabled.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading SAM model: {e}\")\n",
        "            print(f\"   Ensure the model file at {sam_model_path} is valid and dependencies are installed.\")\n",
        "            traceback.print_exc() # Print detailed traceback for debugging\n",
        "            self.sam_predictor = None\n",
        "            self.sam_model_name = None\n",
        "            globals()['ENABLE_SAM'] = False # Disable SAM if model loading fails\n",
        "            print(\"‚ö†Ô∏è SAM functionality has been disabled due to model loading error.\")\n",
        "\n",
        "\n",
        "    def _unload_pipeline(self):\n",
        "        \"\"\"Moves the current pipeline to CPU and clears memory.\"\"\"\n",
        "        if self.current_pipeline is not None:\n",
        "            print(f\"Unloading pipeline: {self.current_model_name} ({self.current_pipeline_type})...\")\n",
        "            try:\n",
        "                # Check if pipeline has 'to' method before calling\n",
        "                if hasattr(self.current_pipeline, 'to'):\n",
        "                    self.current_pipeline.to(\"cpu\")\n",
        "                else:\n",
        "                    print(\"Warning: Pipeline object doesn't have 'to' method for CPU transfer.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error moving pipeline to CPU: {e}\")\n",
        "\n",
        "            # Delete references\n",
        "            pipeline_ref = self.current_pipeline\n",
        "            self.current_pipeline = None\n",
        "            self.current_model_name = None\n",
        "            self.current_pipeline_type = None\n",
        "            del pipeline_ref # Explicitly delete the reference\n",
        "\n",
        "            # Force garbage collection and clear CUDA cache\n",
        "            gc.collect()\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            print(\"Pipeline unloaded and memory cleared.\")\n",
        "        else:\n",
        "             # If no pipeline loaded, still try to clear cache just in case\n",
        "             gc.collect()\n",
        "             if self.device == \"cuda\":\n",
        "                 torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    def _load_pipeline(self, model_name, task_type):\n",
        "        \"\"\"\n",
        "        Loads the appropriate diffusion pipeline for the given model and task.\n",
        "        Manages VRAM by unloading the previous pipeline.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): The display name of the model from PREDEFINED_MODELS.\n",
        "            task_type (str): The type of task ('txt2img', 'inpaint', 'img2img').\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the pipeline was loaded successfully, False otherwise.\n",
        "        \"\"\"\n",
        "        # Ensure model_manager is valid\n",
        "        if not hasattr(self, 'model_manager') or self.model_manager is None:\n",
        "             print(\"‚ùå Cannot load pipeline: ModelManager is not available.\")\n",
        "             return False\n",
        "\n",
        "        # Check if requested pipeline is already loaded\n",
        "        if model_name == self.current_model_name and task_type == self.current_pipeline_type and self.current_pipeline:\n",
        "            print(f\"Pipeline '{model_name}' for task '{task_type}' already loaded.\")\n",
        "            # Ensure it's on the correct device (might have been moved to CPU)\n",
        "            if hasattr(self.current_pipeline, 'device') and str(self.current_pipeline.device) != self.device:\n",
        "                 print(f\"Moving existing pipeline back to {self.device}...\")\n",
        "                 try:\n",
        "                     self.current_pipeline.to(self.device)\n",
        "                     print(\"‚úÖ Pipeline moved back to active device.\")\n",
        "                 except Exception as e:\n",
        "                     print(f\"‚ùå Failed to move existing pipeline to {self.device}: {e}\")\n",
        "                     # Force unload/reload if moving fails\n",
        "                     self._unload_pipeline()\n",
        "                     # Continue to reload logic below...\n",
        "                 else:\n",
        "                     return True # Already loaded and on correct device\n",
        "            else:\n",
        "                 return True # Already loaded and on correct device (or device check not possible)\n",
        "\n",
        "\n",
        "        # Get model path using model manager\n",
        "        model_path = self.model_manager.get_model_path(model_name)\n",
        "        if not model_path:\n",
        "            print(f\"‚ùå Model '{model_name}' not found locally via ModelManager. Please download it first.\")\n",
        "            return False\n",
        "        # Further check if path actually exists (get_model_path might return theoretical path)\n",
        "        if not os.path.exists(model_path):\n",
        "             print(f\"‚ùå Model path found by manager but does not exist on disk: {model_path}\")\n",
        "             return False\n",
        "\n",
        "\n",
        "        # Unload previous pipeline before loading new one\n",
        "        self._unload_pipeline()\n",
        "\n",
        "        print(f\"\\n--- Loading Pipeline ---\")\n",
        "        print(f\"Model: {model_name}\")\n",
        "        print(f\"Task: {task_type}\")\n",
        "        print(f\"Path: {model_path}\")\n",
        "\n",
        "        pipeline_class = None\n",
        "        load_method_name = None # Store name for logging\n",
        "        load_args = {}\n",
        "\n",
        "        # Determine pipeline class based on task\n",
        "        if task_type == 'txt2img': pipeline_class = StableDiffusionPipeline\n",
        "        elif task_type == 'inpaint': pipeline_class = StableDiffusionInpaintPipeline\n",
        "        elif task_type == 'img2img': pipeline_class = StableDiffusionImg2ImgPipeline\n",
        "        else:\n",
        "            print(f\"‚ùå Unsupported task type: {task_type}\")\n",
        "            return False\n",
        "\n",
        "        # Determine load method based on path type (directory vs file)\n",
        "        if os.path.isdir(model_path):\n",
        "            load_method = pipeline_class.from_pretrained\n",
        "            load_method_name = \"from_pretrained\"\n",
        "            load_args['pretrained_model_name_or_path'] = model_path\n",
        "        elif os.path.isfile(model_path) and (model_path.endswith(\".safetensors\") or model_path.endswith(\".ckpt\")):\n",
        "            load_method = pipeline_class.from_single_file\n",
        "            load_method_name = \"from_single_file\"\n",
        "            load_args['pretrained_model_link_or_path'] = model_path\n",
        "            # Add safety_checker=None for single file loads if needed, depends on diffusers version\n",
        "            # load_args['safety_checker'] = None\n",
        "        else:\n",
        "            print(f\"‚ùå Cannot determine load method for path type: {model_path}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            print(f\"Loading pipeline using: {load_method_name}...\")\n",
        "            start_time = time.time()\n",
        "            # Common arguments for loading\n",
        "            common_load_args = {\n",
        "                \"torch_dtype\": torch.float16,\n",
        "                # Add safety checker args if needed, often disabled for custom models\n",
        "                # \"safety_checker\": None,\n",
        "                # \"requires_safety_checker\": False,\n",
        "            }\n",
        "            # Merge specific args with common args\n",
        "            final_load_args = {**load_args, **common_load_args}\n",
        "\n",
        "            pipeline = load_method(**final_load_args)\n",
        "            pipeline.to(self.device)\n",
        "\n",
        "            # Optional: Enable optimizations if available and desired\n",
        "            # try:\n",
        "            #     pipeline.enable_xformers_memory_efficient_attention()\n",
        "            #     print(\"Enabled xformers memory efficient attention.\")\n",
        "            # except Exception:\n",
        "            #     try:\n",
        "            #          # Fallback for newer diffusers/torch versions\n",
        "            #          import torch.nn.functional as F\n",
        "            #          pipeline.enable_attention_slicing()\n",
        "            #          print(\"Enabled attention slicing (fallback).\")\n",
        "            #     except Exception as e_opt:\n",
        "            #          print(f\"Could not enable memory optimizations: {e_opt}\")\n",
        "\n",
        "\n",
        "            self.current_pipeline = pipeline\n",
        "            self.current_model_name = model_name\n",
        "            self.current_pipeline_type = task_type\n",
        "            end_time = time.time()\n",
        "            print(f\"‚úÖ Pipeline loaded successfully in {end_time - start_time:.2f} seconds.\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load pipeline '{model_name}' (Task: {task_type}, Method: {load_method_name}). Error:\")\n",
        "            traceback.print_exc() # Print detailed traceback\n",
        "            self.current_pipeline = None\n",
        "            self.current_model_name = None\n",
        "            self.current_pipeline_type = None\n",
        "            # Attempt cleanup again\n",
        "            gc.collect()\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            return False\n",
        "\n",
        "    def _get_scheduler(self, scheduler_name):\n",
        "        \"\"\"Gets and configures a scheduler instance.\"\"\"\n",
        "        if not self.current_pipeline:\n",
        "            print(\"‚ùå Cannot get scheduler, no pipeline loaded.\")\n",
        "            return None\n",
        "        # Ensure pipeline has scheduler attribute\n",
        "        if not hasattr(self.current_pipeline, 'scheduler'):\n",
        "             print(\"‚ùå Current pipeline does not have a scheduler attribute.\")\n",
        "             return None\n",
        "\n",
        "\n",
        "        scheduler_class = AVAILABLE_SCHEDULERS.get(scheduler_name)\n",
        "        if not scheduler_class:\n",
        "            print(f\"‚ö†Ô∏è Scheduler '{scheduler_name}' not found in AVAILABLE_SCHEDULERS. Using default {DEFAULT_SCHEDULER}.\")\n",
        "            scheduler_class = AVAILABLE_SCHEDULERS.get(DEFAULT_SCHEDULER)\n",
        "            if not scheduler_class: # Should not happen if default is in dict\n",
        "                 print(f\"‚ùå Default scheduler {DEFAULT_SCHEDULER} also not found!\")\n",
        "                 return None\n",
        "\n",
        "        try:\n",
        "            # Load scheduler config from the pipeline's current scheduler\n",
        "            # This preserves settings like beta schedules etc.\n",
        "            scheduler = scheduler_class.from_config(self.current_pipeline.scheduler.config)\n",
        "            # Assign the new scheduler instance to the pipeline\n",
        "            self.current_pipeline.scheduler = scheduler\n",
        "            print(f\"Using scheduler: {scheduler_name}\")\n",
        "            return scheduler\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error setting scheduler '{scheduler_name}': {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def _preprocess_image(self, image, target_size=512, divisible_by=8):\n",
        "        \"\"\"Converts and resizes PIL image for Stable Diffusion, ensuring divisibility.\"\"\"\n",
        "        if not isinstance(image, Image.Image):\n",
        "             # Try to load if it's a path? No, expect PIL image.\n",
        "            raise ValueError(\"Input 'image' must be a PIL Image object.\")\n",
        "\n",
        "        try:\n",
        "            # Ensure RGB format\n",
        "            if image.mode != \"RGB\":\n",
        "                 print(f\"Converting image from mode {image.mode} to RGB.\")\n",
        "                 image = image.convert(\"RGB\")\n",
        "\n",
        "            # Ensure dimensions are divisible by required number\n",
        "            width, height = image.size\n",
        "            new_width = width - (width % divisible_by)\n",
        "            new_height = height - (height % divisible_by)\n",
        "\n",
        "            # Handle cases where rounding down makes dimensions zero\n",
        "            if new_width == 0: new_width = divisible_by\n",
        "            if new_height == 0: new_height = divisible_by\n",
        "\n",
        "            if new_width != width or new_height != height:\n",
        "                print(f\"Resizing image from ({width}, {height}) to ({new_width}, {new_height}) to be divisible by {divisible_by}.\")\n",
        "                image = image.resize((new_width, new_height), Image.Resampling.LANCZOS) # High quality downsampling\n",
        "\n",
        "            return image\n",
        "        except Exception as e:\n",
        "             print(f\"‚ùå Error during image preprocessing: {e}\")\n",
        "             raise # Re-raise the exception\n",
        "\n",
        "\n",
        "    def _preprocess_mask(self, mask, target_image):\n",
        "        \"\"\"Converts, resizes mask for inpainting. Expects white=inpaint area.\"\"\"\n",
        "        if not isinstance(mask, Image.Image):\n",
        "            raise ValueError(\"Input 'mask' must be a PIL Image object.\")\n",
        "        if not isinstance(target_image, Image.Image):\n",
        "             raise ValueError(\"Input 'target_image' must be a PIL Image object.\")\n",
        "\n",
        "        try:\n",
        "            target_width, target_height = target_image.size\n",
        "\n",
        "            # Ensure mask matches target image size\n",
        "            if mask.size != target_image.size:\n",
        "                print(f\"Resizing mask from {mask.size} to {target_image.size} to match image.\")\n",
        "                mask = mask.resize((target_width, target_height), Image.Resampling.NEAREST) # Use NEAREST for sharp edges\n",
        "\n",
        "            # Convert mask to grayscale ('L') for processing\n",
        "            if mask.mode != 'L':\n",
        "                 mask = mask.convert('L')\n",
        "\n",
        "            # Binarize the mask: Ensure it's only 0 (black) and 255 (white)\n",
        "            # Pixels > 127 become 255 (white), others become 0 (black)\n",
        "            # This assumes white is the area to inpaint.\n",
        "            threshold = 127\n",
        "            mask = mask.point(lambda p: 255 if p > threshold else 0)\n",
        "\n",
        "\n",
        "            # Convert final mask to RGB for the pipeline (most expect RGB mask)\n",
        "            mask = mask.convert('RGB')\n",
        "\n",
        "            return mask\n",
        "        except Exception as e:\n",
        "             print(f\"‚ùå Error during mask preprocessing: {e}\")\n",
        "             raise # Re-raise the exception\n",
        "\n",
        "    def _save_image(self, image, prompt=\"\"):\n",
        "        \"\"\"Saves the generated PIL image to the output directory.\"\"\"\n",
        "        # Ensure output path exists\n",
        "        try:\n",
        "             os.makedirs(self.output_path, exist_ok=True)\n",
        "        except OSError as e:\n",
        "             print(f\"‚ùå Error creating output directory {self.output_path}: {e}\")\n",
        "             return None # Cannot save if directory fails\n",
        "\n",
        "        try:\n",
        "            # Create a filename\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            # Sanitize prompt for filename\n",
        "            safe_prompt = \"\".join(c if c.isalnum() or c in (' ', '_', '-') else '_' for c in prompt).strip()\n",
        "            safe_prompt = safe_prompt[:50] # Limit length\n",
        "            if not safe_prompt: safe_prompt = \"generated_image\" # Fallback if prompt is empty/unusable\n",
        "\n",
        "            # Add random element to prevent collisions on identical prompts/timestamps\n",
        "            rand_id = random.randint(1000, 9999)\n",
        "            filename = f\"{timestamp}_{safe_prompt}_{rand_id}.png\"\n",
        "            filepath = os.path.join(self.output_path, filename)\n",
        "\n",
        "            # Save the image\n",
        "            image.save(filepath, \"PNG\")\n",
        "            print(f\"‚úÖ Image saved to: {filepath}\")\n",
        "            return filepath\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error saving image to {self.output_path}: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    # --- Core Generation Methods ---\n",
        "\n",
        "    def text_to_image(self, model_name, prompt, negative_prompt=\"\", guidance_scale=7.5,\n",
        "                      num_inference_steps=30, seed=None, width=512, height=512,\n",
        "                      scheduler_name=DEFAULT_SCHEDULER):\n",
        "        \"\"\"Generates an image from text prompts.\"\"\"\n",
        "        print(f\"\\n--- Task: Text-to-Image ---\")\n",
        "        if not self._load_pipeline(model_name, 'txt2img'):\n",
        "            return None, None, seed # Return consistent tuple on failure\n",
        "\n",
        "        if not self._get_scheduler(scheduler_name):\n",
        "             print(\"‚ö†Ô∏è Failed to set scheduler. Proceeding with pipeline's default.\")\n",
        "             # Decide if this is critical - for now, we proceed\n",
        "\n",
        "        # Validate dimensions (must be divisible by 8)\n",
        "        proc_width = (width // 8) * 8\n",
        "        proc_height = (height // 8) * 8\n",
        "        if proc_width == 0 or proc_height == 0:\n",
        "             print(f\"‚ùå Invalid dimensions after ensuring divisibility by 8: {width}x{height} -> {proc_width}x{proc_height}\")\n",
        "             return None, None, seed\n",
        "        if proc_width != width or proc_height != height:\n",
        "             print(f\"Adjusting dimensions to be divisible by 8: {width}x{height} -> {proc_width}x{proc_height}\")\n",
        "             width, height = proc_width, proc_height\n",
        "\n",
        "\n",
        "        # Set seed for reproducibility\n",
        "        generator = torch.Generator(device=self.device)\n",
        "        if seed is None or seed == -1 or not isinstance(seed, int):\n",
        "            seed = random.randint(0, 2**32 - 1) # Generate a valid seed\n",
        "        generator.manual_seed(seed)\n",
        "        print(f\"Using Seed: {seed}\")\n",
        "        print(f\"Parameters: Steps={num_inference_steps}, CFG={guidance_scale}, Size={width}x{height}, Scheduler={scheduler_name}\")\n",
        "\n",
        "        output_image = None\n",
        "        saved_path = None\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            print(\"Generating text-to-image...\")\n",
        "            # Ensure pipeline is on the correct device before inference\n",
        "            self.current_pipeline.to(self.device)\n",
        "            with torch.inference_mode(): # Use inference mode for efficiency\n",
        "                result = self.current_pipeline(\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    generator=generator,\n",
        "                    width=width,\n",
        "                    height=height,\n",
        "                ) # Add error callback? progress callback?\n",
        "            output_image = result.images[0]\n",
        "            end_time = time.time()\n",
        "            print(f\"Image generated in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "            # Save the image\n",
        "            saved_path = self._save_image(output_image, prompt)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during text-to-image generation: {e}\")\n",
        "            traceback.print_exc()\n",
        "            # Attempt to unload pipeline to free memory after error\n",
        "            self._unload_pipeline()\n",
        "\n",
        "        # Return image, path, and seed used (even on failure, return seed)\n",
        "        return output_image, saved_path, seed\n",
        "\n",
        "\n",
        "    def inpaint(self, model_name, prompt, negative_prompt=\"\", base_image=None, mask_image=None,\n",
        "                guidance_scale=7.5, num_inference_steps=50, seed=None,\n",
        "                scheduler_name=DEFAULT_SCHEDULER, strength=0.8):\n",
        "        \"\"\"Fills masked areas of an image based on prompts.\"\"\"\n",
        "        print(f\"\\n--- Task: Inpainting ---\")\n",
        "        if base_image is None or mask_image is None:\n",
        "            print(\"‚ùå Base image and mask image are required for inpainting.\")\n",
        "            return None, None, seed\n",
        "\n",
        "        if not self._load_pipeline(model_name, 'inpaint'):\n",
        "            return None, None, seed\n",
        "\n",
        "        if not self._get_scheduler(scheduler_name):\n",
        "             print(\"‚ö†Ô∏è Failed to set scheduler. Proceeding with pipeline's default.\")\n",
        "\n",
        "        # Preprocess images\n",
        "        processed_image = None\n",
        "        processed_mask = None\n",
        "        try:\n",
        "            print(\"Preprocessing images for inpainting...\")\n",
        "            # Ensure base_image is divisible by 8\n",
        "            processed_image = self._preprocess_image(base_image, divisible_by=8)\n",
        "            # Ensure mask matches processed image size and format (white=inpaint)\n",
        "            processed_mask = self._preprocess_mask(mask_image, processed_image)\n",
        "            print(f\"Processed image size: {processed_image.size}, Mask size: {processed_mask.size}\")\n",
        "        except Exception as e:\n",
        "             print(f\"‚ùå Error during image/mask preprocessing for inpainting: {e}\")\n",
        "             traceback.print_exc()\n",
        "             return None, None, seed\n",
        "\n",
        "        # Set seed\n",
        "        generator = torch.Generator(device=self.device)\n",
        "        if seed is None or seed == -1 or not isinstance(seed, int):\n",
        "            seed = random.randint(0, 2**32 - 1)\n",
        "        generator.manual_seed(seed)\n",
        "        print(f\"Using Seed: {seed}\")\n",
        "        print(f\"Parameters: Steps={num_inference_steps}, CFG={guidance_scale}, Strength={strength}, Scheduler={scheduler_name}\")\n",
        "\n",
        "\n",
        "        output_image = None\n",
        "        saved_path = None\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            print(\"Generating inpainting...\")\n",
        "            # Ensure pipeline is on the correct device\n",
        "            self.current_pipeline.to(self.device)\n",
        "            with torch.inference_mode():\n",
        "                result = self.current_pipeline(\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    image=processed_image, # Use preprocessed image\n",
        "                    mask_image=processed_mask, # Use preprocessed mask\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    generator=generator,\n",
        "                    strength=strength,\n",
        "                    # width=processed_image.width, # Usually inferred\n",
        "                    # height=processed_image.height,\n",
        "                )\n",
        "            output_image = result.images[0]\n",
        "            end_time = time.time()\n",
        "            print(f\"Inpainting generated in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "            # Save the image\n",
        "            saved_path = self._save_image(output_image, prompt + \"_inpainted\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during inpainting generation: {e}\")\n",
        "            traceback.print_exc()\n",
        "            self._unload_pipeline() # Attempt to free memory\n",
        "\n",
        "        return output_image, saved_path, seed\n",
        "\n",
        "\n",
        "    def outpaint(self, model_name, prompt, negative_prompt=\"\", base_image=None,\n",
        "                 pixels_to_expand=128, expand_direction=\"all\", # all, top, bottom, left, right\n",
        "                 guidance_scale=7.5, num_inference_steps=50, seed=None,\n",
        "                 scheduler_name=DEFAULT_SCHEDULER, strength=0.95): # Often need high strength for outpaint\n",
        "        \"\"\"Expands an image outwards using the inpainting pipeline.\"\"\"\n",
        "        print(f\"\\n--- Task: Outpainting ---\")\n",
        "        if base_image is None:\n",
        "            print(\"‚ùå Base image is required for outpainting.\")\n",
        "            return None, None, seed\n",
        "\n",
        "        print(f\"Direction: {expand_direction}, Pixels: {pixels_to_expand}\")\n",
        "        expanded_image = None\n",
        "        mask_rgb = None\n",
        "        orig_width, orig_height = base_image.size\n",
        "\n",
        "        try:\n",
        "             print(\"Preparing canvas and mask for outpainting...\")\n",
        "             # 1. Ensure base image is RGB\n",
        "             image = base_image.convert(\"RGB\")\n",
        "\n",
        "             # 2. Calculate padding and new dimensions\n",
        "             pad_left, pad_right, pad_top, pad_bottom = 0, 0, 0, 0\n",
        "             if expand_direction in [\"all\", \"left\"]: pad_left = pixels_to_expand\n",
        "             if expand_direction in [\"all\", \"right\"]: pad_right = pixels_to_expand\n",
        "             if expand_direction in [\"all\", \"top\"]: pad_top = pixels_to_expand\n",
        "             if expand_direction in [\"all\", \"bottom\"]: pad_bottom = pixels_to_expand\n",
        "\n",
        "             new_width = orig_width + pad_left + pad_right\n",
        "             new_height = orig_height + pad_top + pad_bottom\n",
        "\n",
        "             # Ensure new dimensions are divisible by 8\n",
        "             final_width = max( ((new_width + 7) // 8) * 8, 8) # Ensure at least 8x8\n",
        "             final_height = max( ((new_height + 7) // 8) * 8, 8)\n",
        "\n",
        "             # Adjust padding to match final divisible dimensions\n",
        "             width_diff = final_width - new_width\n",
        "             height_diff = final_height - new_height\n",
        "             # Distribute extra padding (e.g., add to right/bottom)\n",
        "             pad_right += width_diff\n",
        "             pad_bottom += height_diff\n",
        "             new_width, new_height = final_width, final_height\n",
        "\n",
        "             if new_width <= orig_width and new_height <= orig_height:\n",
        "                  print(\"‚ö†Ô∏è Calculated expansion results in image size not increasing. Check parameters.\")\n",
        "                  # Fallback: Add minimum padding if direction was specified\n",
        "                  if pad_left > 0: pad_left = max(pad_left, 8)\n",
        "                  if pad_right > 0: pad_right = max(pad_right, 8)\n",
        "                  if pad_top > 0: pad_top = max(pad_top, 8)\n",
        "                  if pad_bottom > 0: pad_bottom = max(pad_bottom, 8)\n",
        "                  # Recalculate based on minimum padding\n",
        "                  new_width = orig_width + pad_left + pad_right\n",
        "                  new_height = orig_height + pad_top + pad_bottom\n",
        "                  new_width = max( ((new_width + 7) // 8) * 8, 8)\n",
        "                  new_height = max( ((new_height + 7) // 8) * 8, 8)\n",
        "\n",
        "\n",
        "             print(f\"Expanding to: {new_width}x{new_height}\")\n",
        "\n",
        "             # 3. Create expanded canvas (fill with average color or noise?)\n",
        "             # Simple gray fill for now\n",
        "             expanded_image = Image.new(\"RGB\", (new_width, new_height), (127, 127, 127))\n",
        "             expanded_image.paste(image, (pad_left, pad_top))\n",
        "\n",
        "             # 4. Create the mask (white in the expanded areas, black in original area)\n",
        "             mask = Image.new(\"L\", (new_width, new_height), 255) # Start with white\n",
        "             mask_paste_black = Image.new(\"L\", (orig_width, orig_height), 0) # Black rectangle\n",
        "             mask.paste(mask_paste_black, (pad_left, pad_top))\n",
        "\n",
        "             # Convert mask to RGB for pipeline\n",
        "             mask_rgb = mask.convert(\"RGB\")\n",
        "             print(\"Canvas and mask prepared.\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"‚ùå Error during outpainting preparation: {e}\")\n",
        "             traceback.print_exc()\n",
        "             return None, None, seed\n",
        "\n",
        "        # --- Call Inpainting Pipeline ---\n",
        "        print(\"Calling inpaint pipeline for outpainting task...\")\n",
        "        # Pass the prepared expanded image and mask\n",
        "        return self.inpaint(\n",
        "            model_name=model_name,\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            base_image=expanded_image, # Pass the expanded canvas\n",
        "            mask_image=mask_rgb,       # Pass the mask covering new areas\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            seed=seed,\n",
        "            scheduler_name=scheduler_name,\n",
        "            strength=strength # Strength is important for outpainting\n",
        "        )\n",
        "\n",
        "\n",
        "    # --- SAM Mask Generation Method ---\n",
        "\n",
        "    def predict_sam_mask(self, input_image, input_points):\n",
        "        \"\"\"\n",
        "        Generates a segmentation mask using SAM based on input points.\n",
        "\n",
        "        Args:\n",
        "            input_image (PIL.Image): The image to segment.\n",
        "            input_points (list): A list of (x, y) tuples representing click points.\n",
        "\n",
        "        Returns:\n",
        "            PIL.Image: A binary mask (mode 'L', 0=background, 255=mask) or None if failed.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Task: SAM Mask Prediction ---\")\n",
        "        # Check if SAM is enabled *and* predictor is initialized\n",
        "        if not globals().get('ENABLE_SAM', False) or not self.sam_predictor:\n",
        "            print(\"‚ùå SAM is not enabled or initialized. Cannot generate mask.\")\n",
        "            # Try to initialize if enabled but not initialized yet\n",
        "            if globals().get('ENABLE_SAM', False) and not self.sam_predictor:\n",
        "                 print(\"Attempting to initialize SAM predictor now...\")\n",
        "                 self._initialize_sam()\n",
        "                 # Check again\n",
        "                 if not self.sam_predictor:\n",
        "                      print(\"‚ùå SAM predictor initialization failed. Cannot proceed.\")\n",
        "                      return None\n",
        "            else:\n",
        "                 return None # SAM not enabled\n",
        "\n",
        "        if not input_points:\n",
        "            print(\"‚ÑπÔ∏è No points provided for SAM prediction.\")\n",
        "            # Return an empty mask matching input image size\n",
        "            try:\n",
        "                 empty_mask = Image.new('L', input_image.size, 0)\n",
        "                 return empty_mask\n",
        "            except Exception:\n",
        "                 return None # Cannot even create empty mask\n",
        "\n",
        "        print(f\"Generating SAM Mask ({len(input_points)} points)\")\n",
        "        final_mask_pil = None\n",
        "        try:\n",
        "            # Convert PIL Image to OpenCV format (BGR uint8)\n",
        "            image_rgb = input_image.convert(\"RGB\")\n",
        "            image_np = np.array(image_rgb)\n",
        "            image_cv2 = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            # Set the image in the SAM predictor\n",
        "            print(\"Setting image in SAM predictor...\")\n",
        "            start_time = time.time()\n",
        "            self.sam_predictor.set_image(image_cv2)\n",
        "            print(f\"Image set in {time.time() - start_time:.2f}s\")\n",
        "\n",
        "            # Format points and labels for SAM predictor\n",
        "            points_np = np.array(input_points)\n",
        "            # Labels: 1 = foreground point (target object), 0 = background point\n",
        "            labels_np = np.ones(len(input_points), dtype=int) # Assume all clicks are targets\n",
        "\n",
        "            print(f\"Predicting mask with points: {input_points}\")\n",
        "            start_time = time.time()\n",
        "            # Predict masks using the points\n",
        "            masks, scores, logits = self.sam_predictor.predict(\n",
        "                point_coords=points_np,\n",
        "                point_labels=labels_np,\n",
        "                multimask_output=True, # Get multiple masks per point set (usually 3)\n",
        "            )\n",
        "            # masks shape: (num_masks, H, W), boolean numpy array\n",
        "            # scores shape: (num_masks,), float, IoU prediction score\n",
        "            print(f\"Prediction done in {time.time() - start_time:.2f}s.\")\n",
        "            print(f\"Found {len(masks)} masks with scores: {[f'{s:.2f}' for s in scores]}\")\n",
        "\n",
        "            # --- Mask Selection/Merging Logic ---\n",
        "            if len(masks) == 0:\n",
        "                 print(\"‚ö†Ô∏è No masks found by SAM for the given points.\")\n",
        "                 # Return an empty (all black) mask\n",
        "                 final_mask_np = np.zeros(image_cv2.shape[:2], dtype=np.uint8) # Match input image dims H, W\n",
        "            else:\n",
        "                 # Option 1: Take the mask with the highest score\n",
        "                 # best_mask_idx = np.argmax(scores)\n",
        "                 # final_mask_np = masks[best_mask_idx]\n",
        "                 # print(f\"Selected best mask (index {best_mask_idx}) with score {scores[best_mask_idx]:.2f}\")\n",
        "\n",
        "                 # Option 2: Combine all masks using logical OR (more inclusive)\n",
        "                 print(\"Combining all predicted masks using logical OR.\")\n",
        "                 final_mask_np = np.logical_or.reduce(masks, axis=0)\n",
        "\n",
        "                 # Convert boolean mask to uint8 (0 or 255)\n",
        "                 final_mask_np = final_mask_np.astype(np.uint8) * 255\n",
        "\n",
        "            # Convert final numpy mask back to PIL Image ('L' mode)\n",
        "            final_mask_pil = Image.fromarray(final_mask_np, mode='L')\n",
        "            print(\"‚úÖ SAM mask generated successfully.\")\n",
        "\n",
        "        except NameError as ne:\n",
        "             print(f\"‚ùå SAM prediction failed: {ne}. Required libraries (cv2, numpy, segment_anything) might be missing or failed to import.\")\n",
        "             traceback.print_exc()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during SAM mask prediction: {e}\")\n",
        "            traceback.print_exc() # Print detailed traceback for debugging\n",
        "\n",
        "        return final_mask_pil # Return PIL mask or None\n",
        "\n",
        "# --- Instantiate Generator ---\n",
        "print(\"\\nInstantiating ImageGenerator...\")\n",
        "image_generator = None\n",
        "# Check if prerequisite variables exist\n",
        "if 'model_manager' in globals() and model_manager is not None:\n",
        "    if 'OUTPUT_PATH' in globals() and isinstance(OUTPUT_PATH, str):\n",
        "        try:\n",
        "            image_generator = ImageGenerator(model_manager, OUTPUT_PATH)\n",
        "            print(\"‚úÖ ImageGenerator instantiated successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to instantiate ImageGenerator: {e}\")\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"‚ùå ERROR: 'OUTPUT_PATH' not found or invalid (expected string path from Cell 3). Cannot instantiate ImageGenerator.\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: 'model_manager' not found or not initialized (expected from Cell 5). Cannot instantiate ImageGenerator.\")\n",
        "\n",
        "# --- Optional: Quick Test ---\n",
        "# (Keep commented out unless specifically testing this cell)\n",
        "# print(\"\\n--- Optional Quick Test ---\")\n",
        "# if image_generator:\n",
        "#      if image_generator.current_pipeline and image_generator.current_model_name:\n",
        "#           print(f\"\\n--- Quick Test: Text-to-Image using pre-loaded model '{image_generator.current_model_name}' ---\")\n",
        "#           test_prompt = \"A photo of an astronaut riding a horse on the moon\"\n",
        "#           img, path, seed = image_generator.text_to_image(\n",
        "#                model_name=image_generator.current_model_name,\n",
        "#                prompt=test_prompt, num_inference_steps=15, width=512, height=512\n",
        "#           )\n",
        "#           if img: print(f\"Quick test successful. Seed: {seed}, Path: {path}\")\n",
        "#           else: print(\"Quick test failed.\")\n",
        "#      else:\n",
        "#           print(\"\\n--- Quick Test: Text-to-Image (requires model download if not pre-loaded) ---\")\n",
        "#           # Select a model known to be defined in PREDEFINED_MODELS\n",
        "#           test_model = \"Deliberate V3 (HF)\" # Or another model name\n",
        "#           print(f\"Attempting test with model: {test_model}\")\n",
        "#           # Ensure model is downloaded first (optional, download_model handles check)\n",
        "#           # model_manager.download_model(test_model)\n",
        "#           test_prompt = \"A watercolor painting of a cozy cabin in the woods, autumn\"\n",
        "#           img, path, seed = image_generator.text_to_image(\n",
        "#                model_name=test_model, prompt=test_prompt, num_inference_steps=15, width=512, height=512\n",
        "#           )\n",
        "#           if img: print(f\"Quick test successful. Seed: {seed}, Path: {path}\")\n",
        "#           else: print(f\"Quick test failed for model {test_model}.\")\n",
        "# else:\n",
        "#      print(\"\\n‚ÑπÔ∏è ImageGenerator not instantiated, skipping quick test.\")\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ Image Generation & Segmentation cell setup complete.\")\n",
        "if image_generator:\n",
        "     print(\"   'image_generator' instance is ready to use.\")\n",
        "     # Report SAM status based on the flag AND predictor state\n",
        "     sam_status = \"Enabled and Initialized\" if globals().get('ENABLE_SAM', False) and image_generator.sam_predictor else \\\n",
        "                  \"Enabled but NOT Initialized (Check Logs)\" if globals().get('ENABLE_SAM', False) else \\\n",
        "                  \"Disabled\"\n",
        "     print(f\"   SAM Status: {sam_status}\")\n",
        "else:\n",
        "     print(\"   ‚ö†Ô∏è ImageGenerator instance could not be created.\")\n"
      ],
      "metadata": {
        "id": "oej4w_cVqAxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}